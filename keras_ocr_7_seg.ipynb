{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras_ocr_7_seg.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrushanth05/BankClassDemo-Java-/blob/main/keras_ocr_7_seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ocr_training_data.zip -d /content/data/"
      ],
      "metadata": {
        "id": "p0KuOLlfFfPa",
        "outputId": "4ffa07f5-8b2b-46aa-845f-86f59fffddaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ocr_training_data.zip\n",
            "replace /content/data/ocr_training_data/0072f880-397b-4c59-9bf7-d6f83c863ef8.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/data/ocr_training_data/0072f880-397b-4c59-9bf7-d6f83c863ef8.png  \n",
            "  inflating: /content/data/ocr_training_data/012a7ff5-76eb-4e55-870b-8c28e58e9d63.png  \n",
            "  inflating: /content/data/ocr_training_data/0183531e-f05f-488b-afba-8a2e39729e91.png  \n",
            "  inflating: /content/data/ocr_training_data/029b56a3-8d19-4376-aa8b-02a90367b307.png  \n",
            "  inflating: /content/data/ocr_training_data/02a51083-81f2-43a7-87a3-7f74d4e89d47.png  \n",
            "  inflating: /content/data/ocr_training_data/03acc103-023e-47c1-b774-46e25f60dc4a.png  \n",
            "  inflating: /content/data/ocr_training_data/043edcfa-8038-40d9-b22d-99d1515b41bc.png  \n",
            "  inflating: /content/data/ocr_training_data/04c7bf0e-583e-43d0-a989-41b6ddf78b28.png  \n",
            "  inflating: /content/data/ocr_training_data/06818bdf-9ac9-4cb2-8e12-1374fd9628af.png  \n",
            "  inflating: /content/data/ocr_training_data/078f7298-1751-43fb-90d3-743715f028cf.png  \n",
            "  inflating: /content/data/ocr_training_data/08c91ae7-4dd0-47c3-a9f4-f3f365abca11.png  \n",
            "  inflating: /content/data/ocr_training_data/091718eb-be08-48df-ade3-8f96e7d5a723.png  \n",
            "  inflating: /content/data/ocr_training_data/099066b8-aca2-4083-bac6-91bf5b03aa98.png  \n",
            "  inflating: /content/data/ocr_training_data/0a4c38a6-fd25-4711-8eac-149d30f52ecc.png  \n",
            "  inflating: /content/data/ocr_training_data/0bc0a94a-65c3-4734-8e4b-c14a4852e244.png  \n",
            "  inflating: /content/data/ocr_training_data/0c4e3422-2b0e-49fd-a3fe-e8890604b09e.png  \n",
            "  inflating: /content/data/ocr_training_data/0e13389d-9814-4037-8eb7-5d5f03cd8eb5.png  \n",
            "  inflating: /content/data/ocr_training_data/0e2c8a3d-4b3b-4952-b9e2-bcce7873a68e.png  \n",
            "  inflating: /content/data/ocr_training_data/0e3bf453-bcbe-4c04-a5c5-f6b5d14b06b7.png  \n",
            "  inflating: /content/data/ocr_training_data/0f3715e3-09af-4e6d-b4dd-c66c09e0774a.png  \n",
            "  inflating: /content/data/ocr_training_data/10316ef3-d00b-4fb1-918b-128c756bd0b2.png  \n",
            "  inflating: /content/data/ocr_training_data/103b89b2-8d1e-4523-be29-0e3a77e210e1.png  \n",
            "  inflating: /content/data/ocr_training_data/104973d7-dc47-42e3-b5ba-696717e15146.png  \n",
            "  inflating: /content/data/ocr_training_data/117c40d1-ead2-4b98-a363-e4a74ebd9232.png  \n",
            "  inflating: /content/data/ocr_training_data/1238027f-13ca-451f-90fa-fffdd2282679.png  \n",
            "  inflating: /content/data/ocr_training_data/13031625-7c2c-4c5d-b069-d647c2d071cf.png  \n",
            "  inflating: /content/data/ocr_training_data/16c52a8a-379f-4fc2-8d15-de5eae5b865a.png  \n",
            "  inflating: /content/data/ocr_training_data/174f337c-711a-48d6-93ef-295f5c81b1b7.png  \n",
            "  inflating: /content/data/ocr_training_data/185b8fb4-4821-44c0-b092-832545376398.png  \n",
            "  inflating: /content/data/ocr_training_data/1a4a2808-bed8-4a2a-b3cb-3cf21d5bffb9.png  \n",
            "  inflating: /content/data/ocr_training_data/1a89badc-32ea-4338-a742-f1bef0cc37ff.png  \n",
            "  inflating: /content/data/ocr_training_data/1ad55cbc-5ab3-45e9-a499-babf2ec757db.png  \n",
            "  inflating: /content/data/ocr_training_data/1b6a0703-3744-479d-81d9-ffd8b960e813.png  \n",
            "  inflating: /content/data/ocr_training_data/1c78fc46-cbe1-4058-8cc4-8a4a79e8177d.png  \n",
            "  inflating: /content/data/ocr_training_data/1dea9e6e-e9bc-47e7-9cc8-093fcdc59a59.png  \n",
            "  inflating: /content/data/ocr_training_data/1eeedbb1-c7ea-435b-a223-247962958984.png  \n",
            "  inflating: /content/data/ocr_training_data/1f9ebe8a-908a-4d9f-a3c8-29de8288307c.png  \n",
            "  inflating: /content/data/ocr_training_data/1fe837ea-e517-407b-a97f-427fedff3c26.png  \n",
            "  inflating: /content/data/ocr_training_data/20ae8610-7dff-45a3-916b-f040b7bad775.png  \n",
            "  inflating: /content/data/ocr_training_data/21d9eb86-a655-4d66-aff4-ffe6b993052f.png  \n",
            "  inflating: /content/data/ocr_training_data/21fd0822-ef71-4519-bf98-3be60e8a0c4a.png  \n",
            "  inflating: /content/data/ocr_training_data/2277aac4-ec06-4d5e-8894-6966b19d742f.png  \n",
            "  inflating: /content/data/ocr_training_data/23122019DAE01QMR100012657.jpeg  \n",
            "  inflating: /content/data/ocr_training_data/23122019DAE01QMR100016077.jpeg  \n",
            "  inflating: /content/data/ocr_training_data/2348d29f-1e5c-4a6c-ba0f-08a8b86ea6b6.png  \n",
            "  inflating: /content/data/ocr_training_data/23a7458a-0688-4320-bec9-ea8a21ddc8e4.png  \n",
            "  inflating: /content/data/ocr_training_data/253fb047-ee8e-484e-8be3-458bc3d5f386.png  \n",
            "  inflating: /content/data/ocr_training_data/25adb0c3-7033-4fbd-91d6-a8db66b503d1.png  \n",
            "  inflating: /content/data/ocr_training_data/2658364a-6792-4a83-af6f-fd94ca925e68.png  \n",
            "  inflating: /content/data/ocr_training_data/27985ce6-9c56-4f03-a10e-786fc7b01f20.png  \n",
            "  inflating: /content/data/ocr_training_data/27ed0bef-6a33-49fd-995c-c033b8ca0c50.png  \n",
            "  inflating: /content/data/ocr_training_data/293c8d42-36d3-4d31-a11e-b9f60344b623.png  \n",
            "  inflating: /content/data/ocr_training_data/29dd0121-4fb5-4733-b2c0-284a2fd5fc58.png  \n",
            "  inflating: /content/data/ocr_training_data/2a57dff8-5742-4b67-a131-2ccd7507e58d.png  \n",
            "  inflating: /content/data/ocr_training_data/2a98b70f-4922-45c3-83a6-91536f92f995.png  \n",
            "  inflating: /content/data/ocr_training_data/2ac0f3f2-c04b-4567-8004-98924fc37c8c.png  \n",
            "  inflating: /content/data/ocr_training_data/2b0a114f-b239-43e2-a3c5-e07840cb8b4f.png  \n",
            "  inflating: /content/data/ocr_training_data/2bac062a-401c-4dd4-bcb3-e8532fba2ec8.png  \n",
            "  inflating: /content/data/ocr_training_data/2bdb5471-8e7a-4476-acd5-64d1efdaf4c1.png  \n",
            "  inflating: /content/data/ocr_training_data/2c212e37-2a1f-4cb5-bddb-e8a0347f00ed.png  \n",
            "  inflating: /content/data/ocr_training_data/2c97d389-2496-497f-b80b-58b1e630d1e2.png  \n",
            "  inflating: /content/data/ocr_training_data/2d7cf57f-50de-43f0-b2d3-63a1a21eba6c.png  \n",
            "  inflating: /content/data/ocr_training_data/2d88a4e3-073a-4a22-b118-a64683fea6cb.png  \n",
            "  inflating: /content/data/ocr_training_data/2df87eae-4908-42f1-812e-6fd66e440382.png  \n",
            "  inflating: /content/data/ocr_training_data/2f98d68a-50d8-40f6-83a9-4b3669ba5c2c.png  \n",
            "  inflating: /content/data/ocr_training_data/303630fd-b9ff-4510-88b0-554a3ddaab93.png  \n",
            "  inflating: /content/data/ocr_training_data/3052f427-fe5b-48da-9ea0-e020d7f6230e.png  \n",
            "  inflating: /content/data/ocr_training_data/3358f900-b96c-449a-9454-2fd8adbce65d.png  \n",
            "  inflating: /content/data/ocr_training_data/35aafe71-4822-4cdb-9518-c4806eda3adc.png  \n",
            "  inflating: /content/data/ocr_training_data/37a62422-30df-45d9-a40c-a734b24cf312.png  \n",
            "  inflating: /content/data/ocr_training_data/38439e43-24d4-4415-a2c3-49e9fb1be732.png  \n",
            "  inflating: /content/data/ocr_training_data/39a24008-2794-471b-ab26-206be9cf8098.png  \n",
            "  inflating: /content/data/ocr_training_data/3a5b0d83-1dd1-4181-905b-4c8fd1d7dfe8.png  \n",
            "  inflating: /content/data/ocr_training_data/3a69aaa3-15b8-452e-87cf-2d48a7a3fb0e.png  \n",
            "  inflating: /content/data/ocr_training_data/3b20718e-0551-41c9-aa85-890901be557c.png  \n",
            "  inflating: /content/data/ocr_training_data/3b9194de-71f9-4c63-8054-28b8e3178f1b.png  \n",
            "  inflating: /content/data/ocr_training_data/3c61d3bd-2129-413a-ae0e-b3efd330a422.png  \n",
            "  inflating: /content/data/ocr_training_data/3c78dbb2-1649-4624-8eed-08713cb0b578.png  \n",
            "  inflating: /content/data/ocr_training_data/3c9f625c-32e2-466a-8995-434247ff3238.png  \n",
            "  inflating: /content/data/ocr_training_data/3d1d1ea8-0c7a-4ecf-bb9c-41dfb88aa0c7.png  \n",
            "  inflating: /content/data/ocr_training_data/3d2da6d5-a7b3-47a7-b5bb-3dd1f3b4b886.png  \n",
            "  inflating: /content/data/ocr_training_data/3dc7139b-e324-46ee-8d3d-91bb54acf8ad.png  \n",
            "  inflating: /content/data/ocr_training_data/3ead9307-3f82-4700-b86c-86813e793921.png  \n",
            "  inflating: /content/data/ocr_training_data/403bfa47-6a2a-4481-bfa4-3edcc01ddf3a.png  \n",
            "  inflating: /content/data/ocr_training_data/40dc085d-9844-4fe6-a656-5150c8f76820.png  \n",
            "  inflating: /content/data/ocr_training_data/411abe68-d58a-45a0-8c8f-b16f1272c5d7.png  \n",
            "  inflating: /content/data/ocr_training_data/412cb391-fe60-47fc-aa85-bcc9d7046e33.png  \n",
            "  inflating: /content/data/ocr_training_data/41dd0f4e-d1e4-4ec4-9f7d-867568e309df.png  \n",
            "  inflating: /content/data/ocr_training_data/41fcb91b-ce1c-4899-98a4-1a4cf3a29bbf.png  \n",
            "  inflating: /content/data/ocr_training_data/42b65453-782a-4730-b737-c60a6f06e9b3.png  \n",
            "  inflating: /content/data/ocr_training_data/443aa9bc-ff38-450e-a70c-f97c27a81bf7.png  \n",
            "  inflating: /content/data/ocr_training_data/45318acd-5003-45fa-b011-dc26fd4b7969.png  \n",
            "  inflating: /content/data/ocr_training_data/4596bb11-3c38-42e5-bd11-59961bc223a9.png  \n",
            "  inflating: /content/data/ocr_training_data/45aca057-5595-44e0-8541-86fead9cb9e8.png  \n",
            "  inflating: /content/data/ocr_training_data/4696dea2-d2e8-4221-855d-e80ad864fa7e.png  \n",
            "  inflating: /content/data/ocr_training_data/46bea91e-ccef-4c20-93a6-7ae2c1fdbdf8.png  \n",
            "  inflating: /content/data/ocr_training_data/47565249-0725-4e93-9f4c-e87a0ed1e635.png  \n",
            "  inflating: /content/data/ocr_training_data/47941281-0191-4d3d-a697-b3bc26cfb532.png  \n",
            "  inflating: /content/data/ocr_training_data/47b02d34-f86e-468b-8b54-725afec0e600.png  \n",
            "  inflating: /content/data/ocr_training_data/484b3af1-0728-4400-a234-5d60468e68d9.png  \n",
            "  inflating: /content/data/ocr_training_data/499813f6-c4a4-4a9d-a1ed-4fe389ac72a1.png  \n",
            "  inflating: /content/data/ocr_training_data/49ca379c-d22b-4ffe-8209-6c646c74860a.png  \n",
            "  inflating: /content/data/ocr_training_data/4a06db70-f187-46f7-9e81-6621f10c1f4a.png  \n",
            "  inflating: /content/data/ocr_training_data/4a233545-0904-4c94-9403-b2162161739f.png  \n",
            "  inflating: /content/data/ocr_training_data/4a39770f-8f7e-4969-bc69-92b4e0bf4d35.png  \n",
            "  inflating: /content/data/ocr_training_data/4a5ce403-9a68-482e-a18c-281e11e578a8.png  \n",
            "  inflating: /content/data/ocr_training_data/4a6ad475-b76d-4cbb-8521-7461942453aa.png  \n",
            "  inflating: /content/data/ocr_training_data/4b90a088-2f5e-47c7-b3fc-15b60a8f1e71.png  \n",
            "  inflating: /content/data/ocr_training_data/4c25a941-9c43-4672-abc4-42f368768432.png  \n",
            "  inflating: /content/data/ocr_training_data/4dc4cc4a-8d41-455e-b44b-0e52290e923e.png  \n",
            "  inflating: /content/data/ocr_training_data/4deab29f-b160-495f-ac62-6cf262b3904c.png  \n",
            "  inflating: /content/data/ocr_training_data/4ea7e153-dbb3-4986-856e-7b2035ca3d16.png  \n",
            "  inflating: /content/data/ocr_training_data/4f6c7aae-0be8-494a-b74f-42c6d44bcafc.png  \n",
            "  inflating: /content/data/ocr_training_data/4fea118a-4371-4c74-8434-7e570d0999a7.png  \n",
            "  inflating: /content/data/ocr_training_data/5000aa5a-d888-410a-91ba-72493406ccfc.png  \n",
            "  inflating: /content/data/ocr_training_data/502eda12-9c88-4801-88ac-eeee2cbfb9e0.png  \n",
            "  inflating: /content/data/ocr_training_data/54723599-b4a5-44c3-ac3a-25c04fbedf3c.png  \n",
            "  inflating: /content/data/ocr_training_data/557de184-820b-4a29-8d3c-987c73759635.png  \n",
            "  inflating: /content/data/ocr_training_data/559c1c63-f1b0-4267-80d2-6e206a8dc9df.png  \n",
            "  inflating: /content/data/ocr_training_data/55afd1bf-2cea-4491-9a36-e57bf658d27d.png  \n",
            "  inflating: /content/data/ocr_training_data/55b7d8c0-4d31-44a5-9635-3c237702cdce.png  \n",
            "  inflating: /content/data/ocr_training_data/56022f48-5ca8-4726-bab2-8be6ef3c17ad.png  \n",
            "  inflating: /content/data/ocr_training_data/5902e88f-40dd-4660-b683-5b5ce750d814.png  \n",
            "  inflating: /content/data/ocr_training_data/591d9150-b34f-439b-885a-735c7253c826.png  \n",
            "  inflating: /content/data/ocr_training_data/59c959cb-a3fc-4321-8165-5db372a1af1d.png  \n",
            "  inflating: /content/data/ocr_training_data/5a4b7fb1-96a6-4603-b136-e155a2ea6fba.png  \n",
            "  inflating: /content/data/ocr_training_data/5a8c4e4d-7722-43b7-8000-96ed9e0e3b20.png  \n",
            "  inflating: /content/data/ocr_training_data/5b16bbad-b02c-435c-8840-693a452a7dcb.png  \n",
            "  inflating: /content/data/ocr_training_data/5bc4cf2c-4097-4cbc-9828-d2872f2bc84b.png  \n",
            "  inflating: /content/data/ocr_training_data/5bdf5205-d07a-48d1-ac1a-8c1555a6efb5.png  \n",
            "  inflating: /content/data/ocr_training_data/5e241f6d-8239-44c0-902a-93a1f6bb0309.png  \n",
            "  inflating: /content/data/ocr_training_data/5f0c559f-0ba5-4571-9fb3-7e8617e2c2bb.png  \n",
            "  inflating: /content/data/ocr_training_data/5fd0e2d2-184f-4be6-a231-36824116b967.png  \n",
            "  inflating: /content/data/ocr_training_data/60398bbe-4429-4bdb-921e-e113d87d0916.png  \n",
            "  inflating: /content/data/ocr_training_data/6055f85e-53bd-401a-ab83-993aade0af47.png  \n",
            "  inflating: /content/data/ocr_training_data/6102429e-842d-4930-b62f-3ff3540f7e67.png  \n",
            "  inflating: /content/data/ocr_training_data/61cfacbc-19c4-4514-9950-1455c09c5222.png  \n",
            "  inflating: /content/data/ocr_training_data/6297afa0-5595-4655-b249-4657a07bf0fe.png  \n",
            "  inflating: /content/data/ocr_training_data/679b5238-669a-426d-997a-eb9ea059f29c.png  \n",
            "  inflating: /content/data/ocr_training_data/679cd8fd-43f9-43d3-a0ca-c7a0e79a9d08.png  \n",
            "  inflating: /content/data/ocr_training_data/67b23619-6437-44c4-b86b-f1966eeeb12f.png  \n",
            "  inflating: /content/data/ocr_training_data/67d3b363-3890-4afc-b640-1000bf3dc2ed.png  \n",
            "  inflating: /content/data/ocr_training_data/698d6380-94b5-4e31-8a2e-2fdb6f5034c5.png  \n",
            "  inflating: /content/data/ocr_training_data/69a0b30e-125e-4ae8-9182-f441da5836d9.png  \n",
            "  inflating: /content/data/ocr_training_data/69c9c8dd-6be5-4813-9184-e572e9712b2c.png  \n",
            "  inflating: /content/data/ocr_training_data/6a11ec47-34e7-43a3-a84f-b8d6b2d3f9e2.png  \n",
            "  inflating: /content/data/ocr_training_data/6ab6f6fe-8ceb-47d7-992c-4032ac054a47.png  \n",
            "  inflating: /content/data/ocr_training_data/6adf68ae-f886-4586-a98f-cd7bcbb23e54.png  \n",
            "  inflating: /content/data/ocr_training_data/6ae95c80-c0c5-4a1f-8826-07f4b93caa27.png  \n",
            "  inflating: /content/data/ocr_training_data/6aff4d09-817a-4ae1-8be4-cb902c34d889.png  \n",
            "  inflating: /content/data/ocr_training_data/6c5bec3e-68aa-4794-bff1-116e7db7ae0c.png  \n",
            "  inflating: /content/data/ocr_training_data/6dc08817-ac7c-4ae5-a16e-72a566eab0db.png  \n",
            "  inflating: /content/data/ocr_training_data/6dea109d-d71c-46c6-9d2d-b06bc5be97c0.png  \n",
            "  inflating: /content/data/ocr_training_data/6e9c459f-f76a-4234-b9ef-2f5a228d9e34.png  \n",
            "  inflating: /content/data/ocr_training_data/6f8b2685-cbeb-490f-b897-6f30cb2776a5.png  \n",
            "  inflating: /content/data/ocr_training_data/7039bc46-8ea7-4a4f-b774-49bbd32f4022.png  \n",
            "  inflating: /content/data/ocr_training_data/70d05103-20f1-4a09-b588-0a337eee1746.png  \n",
            "  inflating: /content/data/ocr_training_data/7140aea1-ddb6-42ff-a172-52b79200f08a.png  \n",
            "  inflating: /content/data/ocr_training_data/721eb14f-8a93-40b3-90c4-9b6f379c9312.png  \n",
            "  inflating: /content/data/ocr_training_data/72cfcb5e-118d-4792-8fb3-43c58af1cd1d.png  \n",
            "  inflating: /content/data/ocr_training_data/72d2ab66-4bda-46c0-b84a-eae289b30da0.png  \n",
            "  inflating: /content/data/ocr_training_data/7354b09d-e1a9-4495-9ca1-6db96f066c9e.png  \n",
            "  inflating: /content/data/ocr_training_data/73a28690-481d-4308-bb63-c5a2f91b082e.png  \n",
            "  inflating: /content/data/ocr_training_data/75299859-2194-4d7d-898b-dd8c4539562d.png  \n",
            "  inflating: /content/data/ocr_training_data/76c3fbca-bbf5-4eae-82ac-1cf318fd7c30.png  \n",
            "  inflating: /content/data/ocr_training_data/76efa967-b30d-4fab-aa03-937d6e9dd006.png  \n",
            "  inflating: /content/data/ocr_training_data/78229fcb-55d9-437d-a044-216cd7db86d1.png  \n",
            "  inflating: /content/data/ocr_training_data/78ff8f7c-13e8-48ed-9031-ca1cd7998bda.png  \n",
            "  inflating: /content/data/ocr_training_data/7925f409-e735-45e1-a3d6-e2f160ba84e5.png  \n",
            "  inflating: /content/data/ocr_training_data/79f3e59d-d358-48fa-a8bd-798f5b6139d4.png  \n",
            "  inflating: /content/data/ocr_training_data/7d2e884b-ff3f-47b1-a518-2d42e3039934.png  \n",
            "  inflating: /content/data/ocr_training_data/7ecf0a0a-7bc0-4e8f-bc8f-c5510674e1e6.png  \n",
            "  inflating: /content/data/ocr_training_data/7f54ad40-5b86-4582-b2f5-2dc4e29ae335.png  \n",
            "  inflating: /content/data/ocr_training_data/7fb978ab-62df-41d7-96fa-2159fdb5db99.png  \n",
            "  inflating: /content/data/ocr_training_data/80033c60-5e0e-4b8a-980d-aa0fecb9077a.png  \n",
            "  inflating: /content/data/ocr_training_data/8262cbd7-1119-4651-a989-364d33851ea7.png  \n",
            "  inflating: /content/data/ocr_training_data/82cea866-17b2-492a-88cf-2fe81d6f1208.png  \n",
            "  inflating: /content/data/ocr_training_data/86469760-7141-4780-af77-fbd578483f44.png  \n",
            "  inflating: /content/data/ocr_training_data/86aaee62-f971-49a2-81ad-2bb4a28cf423.png  \n",
            "  inflating: /content/data/ocr_training_data/8769f65e-39d3-4a58-97ef-2facbc4c24bf.png  \n",
            "  inflating: /content/data/ocr_training_data/88240ed7-7324-4dff-8856-2f3551da015a.png  \n",
            "  inflating: /content/data/ocr_training_data/884f7628-0863-4113-b631-2aca0a087f58.png  \n",
            "  inflating: /content/data/ocr_training_data/88a2c172-c5c0-4c6a-bcd5-b2b83c753824.png  \n",
            "  inflating: /content/data/ocr_training_data/89c328ed-9203-44eb-86c7-57d98eaf7a54.png  \n",
            "  inflating: /content/data/ocr_training_data/89eb40e3-faea-455b-bea1-6a64a4481945.png  \n",
            "  inflating: /content/data/ocr_training_data/8ab11a2f-e37f-422a-94b3-dd664d8a635d.png  \n",
            "  inflating: /content/data/ocr_training_data/8bf5197d-1735-4a9d-a762-6d501287c24b.png  \n",
            "  inflating: /content/data/ocr_training_data/8c564508-4106-4461-b777-5ea4b42a5c5d.png  \n",
            "  inflating: /content/data/ocr_training_data/8c7112fb-01d5-4e3d-b523-f0d63c58e177.png  \n",
            "  inflating: /content/data/ocr_training_data/8d040e7d-dc6d-4e09-b0c7-4a931ac0a17a.png  \n",
            "  inflating: /content/data/ocr_training_data/8d2481db-3bf4-4d20-a4a8-6d04417e8a54.png  \n",
            "  inflating: /content/data/ocr_training_data/8d759cfe-461d-4715-ac1f-968448710ac6.png  \n",
            "  inflating: /content/data/ocr_training_data/8d909744-baa7-401f-b124-7c10e7991a89.png  \n",
            "  inflating: /content/data/ocr_training_data/8fe5787e-5b62-4bd6-8675-cb215b228992.png  \n",
            "  inflating: /content/data/ocr_training_data/908dd586-68bf-4dc4-bd9b-085f2202b4c6.png  \n",
            "  inflating: /content/data/ocr_training_data/9151b243-ac55-47b9-9ab6-018489136b15.png  \n",
            "  inflating: /content/data/ocr_training_data/923d8d25-7555-4e9e-abac-641a74014936.png  \n",
            "  inflating: /content/data/ocr_training_data/93711dba-5fa2-4136-b8c5-3d7330bf23e0.png  \n",
            "  inflating: /content/data/ocr_training_data/955fed90-fefa-40b1-9459-c2df99cb1956.png  \n",
            "  inflating: /content/data/ocr_training_data/9592d6d1-e2c0-4d76-9c1f-27bf4daa83a0.png  \n",
            "  inflating: /content/data/ocr_training_data/959591ae-bdd2-47e5-a05f-1a9e1b7e340c.png  \n",
            "  inflating: /content/data/ocr_training_data/960afb02-8fb1-4fb9-8e6c-1fd56854ca20.png  \n",
            "  inflating: /content/data/ocr_training_data/965597af-ff71-4c5b-af41-0da53a7674a3.png  \n",
            "  inflating: /content/data/ocr_training_data/983a7839-1489-40b9-9293-ebe21af46cfd.png  \n",
            "  inflating: /content/data/ocr_training_data/98ce7c68-fb7c-4601-8133-0fdacea3f9c1.png  \n",
            "  inflating: /content/data/ocr_training_data/9921f87e-20d1-437d-b41c-6819132822c6.png  \n",
            "  inflating: /content/data/ocr_training_data/998ae497-5ae8-40a6-bef7-77f746cec575.png  \n",
            "  inflating: /content/data/ocr_training_data/9b53db35-f512-4348-8f45-b6d29807073c.png  \n",
            "  inflating: /content/data/ocr_training_data/9b7ae883-a9a0-4ec7-8ae2-c5902367f673.png  \n",
            "  inflating: /content/data/ocr_training_data/9c2517b4-ae0f-4700-8e0e-a570ef5c1418.png  \n",
            "  inflating: /content/data/ocr_training_data/9c2c5366-63d1-48dd-a0a2-4169c562c18d.png  \n",
            "  inflating: /content/data/ocr_training_data/9c41708d-5ef9-41c6-bf16-2065c34bc260.png  \n",
            "  inflating: /content/data/ocr_training_data/9c9469e7-e91a-4ce2-863c-cd43d11bc618.png  \n",
            "  inflating: /content/data/ocr_training_data/9d5c34e8-051f-4f77-aa5d-6140f793d8ef.png  \n",
            "  inflating: /content/data/ocr_training_data/9db5038a-11d8-4f55-a7c6-8a885a5ba841.png  \n",
            "  inflating: /content/data/ocr_training_data/9e6f51bb-2733-475f-b1ff-12844e34c8ac.png  \n",
            "  inflating: /content/data/ocr_training_data/9f0eba1c-8f16-4ac8-9c88-2deb10a8c24b.png  \n",
            "  inflating: /content/data/ocr_training_data/9f3949ec-d8fa-4979-84b2-088209844b62.png  \n",
            "  inflating: /content/data/ocr_training_data/9f66d06b-900f-47e6-b613-9694a4dba01c.png  \n",
            "  inflating: /content/data/ocr_training_data/9f8c9026-ef34-4253-a6a9-3a16174b1001.png  \n",
            "  inflating: /content/data/ocr_training_data/9f91a1b6-46ac-4276-b055-34279fbbdaeb.png  \n",
            "  inflating: /content/data/ocr_training_data/a0e4eed9-aea6-4b0a-8ab3-d82d5283868b.png  \n",
            "  inflating: /content/data/ocr_training_data/a131e96c-720b-4081-bb60-fdb646eccf1b.png  \n",
            "  inflating: /content/data/ocr_training_data/a1668c14-613f-44ce-b094-1ff96751b80a.png  \n",
            "  inflating: /content/data/ocr_training_data/a1b31313-2602-47ce-8a85-554b6740a885.png  \n",
            "  inflating: /content/data/ocr_training_data/a2df6cfc-77ab-4dcf-9149-b8b70d9d1311.png  \n",
            "  inflating: /content/data/ocr_training_data/a33df57b-efcb-4d8b-9cbb-b8d3ed5aadf1.png  \n",
            "  inflating: /content/data/ocr_training_data/a4ad1418-d8d4-4092-9b17-1dea99aae77a.png  \n",
            "  inflating: /content/data/ocr_training_data/a5179ae1-0dbc-4ebd-8e27-e9ce9f417b16.png  \n",
            "  inflating: /content/data/ocr_training_data/a5db5ea3-9476-409d-a8d1-7b757bede49c.png  \n",
            "  inflating: /content/data/ocr_training_data/a5f68703-37e6-457e-ae9f-57fc6b23f1ea.png  \n",
            "  inflating: /content/data/ocr_training_data/a705c267-1921-4360-aace-f443fc7e6ba2.png  \n",
            "  inflating: /content/data/ocr_training_data/a83a15c2-c2d8-4af8-b7a8-052906d62e16.png  \n",
            "  inflating: /content/data/ocr_training_data/aae01a37-49cd-42a6-b661-49aac6f3652f.png  \n",
            "  inflating: /content/data/ocr_training_data/abe2c5ed-1248-4bf4-95a5-538a1e6f95ec.png  \n",
            "  inflating: /content/data/ocr_training_data/ac1125af-9d5c-4b8a-8b33-82d3e7317610.png  \n",
            "  inflating: /content/data/ocr_training_data/acc9eed0-4576-4307-a429-10cc552440fb.png  \n",
            "  inflating: /content/data/ocr_training_data/ad03975d-e674-4746-b577-8ae921f5292a.png  \n",
            "  inflating: /content/data/ocr_training_data/ad10e9c7-3b16-4c1c-97dd-f979056e8856.png  \n",
            "  inflating: /content/data/ocr_training_data/ae1a77c1-1a9a-4d97-a999-a764fea6f8b2.png  \n",
            "  inflating: /content/data/ocr_training_data/ae57c33e-d7c9-41c0-bbef-ef36ba924fb4.png  \n",
            "  inflating: /content/data/ocr_training_data/b103d49c-0263-4e65-b7b7-553e0a806b95.png  \n",
            "  inflating: /content/data/ocr_training_data/b37eba6d-a5be-4317-ac73-ddf71fbe32cd.png  \n",
            "  inflating: /content/data/ocr_training_data/b41be0e0-9f6d-42bc-9b73-fd51a8fd5705.png  \n",
            "  inflating: /content/data/ocr_training_data/b5df9663-efd6-4662-8023-4860b792ab1b.png  \n",
            "  inflating: /content/data/ocr_training_data/b69918ae-fd1d-441f-9741-7e7a86f314ef.png  \n",
            "  inflating: /content/data/ocr_training_data/b86d24b8-608e-46f5-9544-5a0724c0b0a3.png  \n",
            "  inflating: /content/data/ocr_training_data/b94c264e-95aa-4979-9625-2fab84b72c27.png  \n",
            "  inflating: /content/data/ocr_training_data/bb0a6b7f-0214-417a-abe8-ace45b078bf5.png  \n",
            "  inflating: /content/data/ocr_training_data/bb6a6957-912d-4def-9eb5-1073f867f83a.png  \n",
            "  inflating: /content/data/ocr_training_data/bbbe1938-50bf-41c8-9bc0-21b355ade506.png  \n",
            "  inflating: /content/data/ocr_training_data/bcd098e9-5c64-422d-ae15-64931db87f4a.png  \n",
            "  inflating: /content/data/ocr_training_data/bcda3944-a1ae-4550-a141-4e9847dcdae2.png  \n",
            "  inflating: /content/data/ocr_training_data/bf5a280d-9e8d-405c-bec3-773e861c7124.png  \n",
            "  inflating: /content/data/ocr_training_data/bfe1af41-299f-4508-9ce1-173175dce8f1.png  \n",
            "  inflating: /content/data/ocr_training_data/c2825d89-31f3-47cc-ab13-10315121488d.png  \n",
            "  inflating: /content/data/ocr_training_data/c4cd268b-13d1-4185-88de-8e9110b1c0d4.png  \n",
            "  inflating: /content/data/ocr_training_data/c6ce427d-12d3-469f-b6eb-1b9986620c3d.png  \n",
            "  inflating: /content/data/ocr_training_data/c7985632-af26-4b09-b4a6-b14e3c0483be.png  \n",
            "  inflating: /content/data/ocr_training_data/c8031de0-2736-4dbc-8ea4-3710f2d929c3.png  \n",
            "  inflating: /content/data/ocr_training_data/c83dec4d-d67f-4e0b-bf78-afbf3fa63441.png  \n",
            "  inflating: /content/data/ocr_training_data/c8a853e3-a6f9-4927-8db9-99be0e0bacbf.png  \n",
            "  inflating: /content/data/ocr_training_data/c8b3e455-0594-4497-90ea-74b859c301a2.png  \n",
            "  inflating: /content/data/ocr_training_data/c8ee3e63-de17-4b67-bce2-ee7731696f0f.png  \n",
            "  inflating: /content/data/ocr_training_data/ca4adbfd-f6c8-4267-9884-b7c3ef162ab5.png  \n",
            "  inflating: /content/data/ocr_training_data/ca5460b1-a231-49d3-a868-11f0fda7e738.png  \n",
            "  inflating: /content/data/ocr_training_data/ce1cddfd-b19d-462c-8e6d-ed206941d7a8.png  \n",
            "  inflating: /content/data/ocr_training_data/cec2dda9-7499-4f51-92a4-9134ddec7d10.png  \n",
            "  inflating: /content/data/ocr_training_data/cfdfe1c5-5388-4240-b4a6-ef995aa154c8.png  \n",
            "  inflating: /content/data/ocr_training_data/d012cfd9-a358-4172-8883-9449b35e6a87.png  \n",
            "  inflating: /content/data/ocr_training_data/d436e50d-55da-4fbc-9c17-eacb5306c2fe.png  \n",
            "  inflating: /content/data/ocr_training_data/d45b3821-152e-4324-b245-239ea10568d7.png  \n",
            "  inflating: /content/data/ocr_training_data/d4734a30-bce5-4b35-9f5a-4cdf8b24af79.png  \n",
            "  inflating: /content/data/ocr_training_data/d59ec701-5bcb-4771-986a-46920300e62b.png  \n",
            "  inflating: /content/data/ocr_training_data/d601bdad-feda-4d62-a45c-72cdb4a6941a.png  \n",
            "  inflating: /content/data/ocr_training_data/d61232df-40a5-496f-8e92-e9a40fb17439.png  \n",
            "  inflating: /content/data/ocr_training_data/d71d0863-5bad-416b-bbe6-906c828aefff.png  \n",
            "  inflating: /content/data/ocr_training_data/da20c1d6-404c-48de-b0f0-296f258a5965.png  \n",
            "  inflating: /content/data/ocr_training_data/dac6e0c9-8b35-4df9-9867-80b69d739a5e.png  \n",
            "  inflating: /content/data/ocr_training_data/dc012f1d-d5ef-4024-b050-c55bd94ac4d0.png  \n",
            "  inflating: /content/data/ocr_training_data/dca410ce-d158-4124-ba79-074d2393ae5a.png  \n",
            "  inflating: /content/data/ocr_training_data/dcd03294-f69c-4058-bacb-a6b2aa8e93d6.png  \n",
            "  inflating: /content/data/ocr_training_data/df1972fc-7494-4054-8773-f8d032355eac.png  \n",
            "  inflating: /content/data/ocr_training_data/df5a4c83-f531-46ef-9db4-33170600a85a.png  \n",
            "  inflating: /content/data/ocr_training_data/dfe257fc-a497-48fe-8893-bab3d3f2bc5c.png  \n",
            "  inflating: /content/data/ocr_training_data/e0fd61b1-1306-4c28-898b-274c36961222.png  \n",
            "  inflating: /content/data/ocr_training_data/e16164a1-4671-4473-bcf2-fd83a66ce51d.png  \n",
            "  inflating: /content/data/ocr_training_data/e241fed1-d73b-4f8a-a63d-01c4b935bf29.png  \n",
            "  inflating: /content/data/ocr_training_data/e2f50c39-f473-4ae6-80ed-a32e712bfa8f.png  \n",
            "  inflating: /content/data/ocr_training_data/e316bb27-82fa-4382-920d-c69f2389d55a.png  \n",
            "  inflating: /content/data/ocr_training_data/e35f0e77-e657-4464-869b-4c800fca70e0.png  \n",
            "  inflating: /content/data/ocr_training_data/e3975137-3c51-4fb6-a0a8-06cc72e8f8b3.png  \n",
            "  inflating: /content/data/ocr_training_data/e46663fb-ccdf-439b-b4bc-0585fcd2228c.png  \n",
            "  inflating: /content/data/ocr_training_data/e6faa660-a1d3-4179-a949-73835c0c88c6.png  \n",
            "  inflating: /content/data/ocr_training_data/e737682e-5d38-4848-b2d8-c28489f3f8fd.png  \n",
            "  inflating: /content/data/ocr_training_data/e790f9dc-d47d-4fff-bcdb-91b64e6274c2.png  \n",
            "  inflating: /content/data/ocr_training_data/e7928e75-67e5-4d71-9598-7c96371f46e5.png  \n",
            "  inflating: /content/data/ocr_training_data/e7cd98d7-decb-4d96-8a61-9d7ccec65970.png  \n",
            "  inflating: /content/data/ocr_training_data/e806eaa5-01f5-4bc2-b005-f3dabd7b0203.png  \n",
            "  inflating: /content/data/ocr_training_data/e961a20e-5ec7-4995-b376-d34311c66d20.png  \n",
            "  inflating: /content/data/ocr_training_data/e9cbf053-fac8-4809-846f-877d1b3a4795.png  \n",
            "  inflating: /content/data/ocr_training_data/ea18c8c2-2ad1-4c41-aede-ba4466608328.png  \n",
            "  inflating: /content/data/ocr_training_data/eaa2a95c-897f-451a-baa5-c581350546a7.png  \n",
            "  inflating: /content/data/ocr_training_data/eb238b9b-9a90-457c-ac80-ecd30c34896b.png  \n",
            "  inflating: /content/data/ocr_training_data/eb7a683a-b18c-41df-9b54-8fb89e7e8b3a.png  \n",
            "  inflating: /content/data/ocr_training_data/ee93eb75-95a3-48f9-ac9d-37bb601ddf4d.png  \n",
            "  inflating: /content/data/ocr_training_data/f05a316e-802e-4130-9727-cd0150889f81.png  \n",
            "  inflating: /content/data/ocr_training_data/f13d9390-6c9f-4fd9-8549-8946867312da.png  \n",
            "  inflating: /content/data/ocr_training_data/f184f11d-3642-49bc-8c2b-dad3806d857f.png  \n",
            "  inflating: /content/data/ocr_training_data/f27fa1a0-dd1e-4698-919d-f3ba09d4511b.png  \n",
            "  inflating: /content/data/ocr_training_data/f2a9b0c4-e678-4133-9fdb-6ff6d57898e3.png  \n",
            "  inflating: /content/data/ocr_training_data/f4db38fa-d17f-44c9-8a4d-ab46942351c3.png  \n",
            "  inflating: /content/data/ocr_training_data/f5219c9d-b54a-41dc-809f-3c8b8f976889.png  \n",
            "  inflating: /content/data/ocr_training_data/f63a98a8-0eb1-43fc-9bb0-de831161561a.png  \n",
            "  inflating: /content/data/ocr_training_data/f7aec436-2cf2-4957-a3d6-f5842fc10fea.png  \n",
            "  inflating: /content/data/ocr_training_data/f8a9cd02-a220-40bb-b3b9-8c15eb99066e.png  \n",
            "  inflating: /content/data/ocr_training_data/fc5e6006-dcd1-4b2d-802a-3b85778f3e15.png  \n",
            "  inflating: /content/data/ocr_training_data/fd421071-e490-4f6f-a7de-e4386d60aa5d.png  \n",
            "  inflating: /content/data/ocr_training_data/ff91b07b-cf94-49fa-9f43-0000c9d9b031.png  \n",
            "  inflating: /content/data/ocr_training_data/gt.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPQEvP0y_d02"
      },
      "source": [
        "# Fine-tuning keras-ocr for seven segment OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kuzOODGiZdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95acc72-afc4-4471-f612-20fa35fef7e0"
      },
      "source": [
        "!pip install -qq -U git+https://github.com/faustomorales/keras-ocr.git#egg=keras-ocr\n",
        "!pip install -qq imgaug\n",
        "!pip install -qq -U opencv-python # We need the most recent version of OpenCV.\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSWr6nzC_heY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fCivLsTi8qA",
        "outputId": "62095152-0a44-4aba-e72e-068ea2d44ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "import random\n",
        "import string\n",
        "import math\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sklearn.model_selection\n",
        "\n",
        "import keras_ocr\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "`np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ca053ac33b26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imgaug/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# classes/functions, hence always place the other imports below this so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the deprecated stuff gets overwritten as much as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maugmentables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imgaug/imgaug.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# `dtype.type in  NP_FLOAT_TYPES` do not just use `dtype in NP_FLOAT_TYPES` as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# that would fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mNP_FLOAT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mNP_INT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mNP_UINT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__expired_attributes__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0;34mf\"`np.{attr}` was removed in the NumPy 2.0 release. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;34mf\"{__expired_attributes__[attr]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Data"
      ],
      "metadata": {
        "id": "tYToriwwrcvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ocr_training_data\n",
        "!unzip -qq ocr_training_data"
      ],
      "metadata": {
        "id": "h7YbbqrqxTwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './ocr_training_data'\n",
        "gt_file = f'{data_dir}/gt.txt'\n",
        "\n",
        "with open(gt_file) as file:\n",
        "    lines = file.read().splitlines()\n",
        "    train_labels = [(data_dir + '/' + line.split('\\t')[0], None, line.split('\\t')[1]) for line in lines]\n",
        "\n",
        "# train_labels = [(filepath, box, word.lower()) for filepath, box, word in train_labels]\n",
        "# test_labels = [(filepath, box, word.lower()) for filepath, box, word in train_labels]"
      ],
      "metadata": {
        "id": "HwB4wgauwnQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{train_labels} samples')"
      ],
      "metadata": {
        "id": "3cuA6iir3pnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vENhyWB_tWw"
      },
      "source": [
        "We next build our recognizer, using the default options to get a pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbZicDRHjKCe"
      },
      "source": [
        "alphabet = string.digits + string.ascii_lowercase + '.'\n",
        "recognizer = keras_ocr.recognition.Recognizer(alphabet=alphabet)\n",
        "recognizer.compile()\n",
        "print(recognizer.model.input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will resize the images to 200x31 to match the models input"
      ],
      "metadata": {
        "id": "v3MJ8UtBrnA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os, sys\n",
        "\n",
        "dirs = os.listdir(data_dir)\n",
        "h, w = recognizer.model.input_shape[1], recognizer.model.input_shape[2]\n",
        "\n",
        "def resize():\n",
        "  for item in dirs:\n",
        "    if os.path.isfile(data_dir+ '/' + item) and '.png' in item:\n",
        "      im = Image.open(data_dir+ '/' + item)\n",
        "      imResize = im.resize((w, h), Image.BICUBIC)\n",
        "      imResize.save(data_dir + '/' + item, 'PNG')\n",
        "\n",
        "resize()"
      ],
      "metadata": {
        "id": "H5FWOskh_dw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNAYeoVC_xXo"
      },
      "source": [
        "We need to convert our dataset into the format that :code:`keras-ocr` requires. To\n",
        "do that, we have the following, which includes support for an augmenter to\n",
        "generate synthetically altered samples. Note that this code is set up to skip\n",
        "any characters that are not in the recognizer alphabet and that all labels\n",
        "are first converted to lowercase.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
        "# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second\n",
        "# image.\n",
        "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "\n",
        "augmenter = iaa.Sequential(\n",
        "    [\n",
        "        #\n",
        "        # Apply the following augmenters to most images.\n",
        "        #\n",
        "        # iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
        "        # iaa.Flipud(0.2), # vertically flip 20% of all images\n",
        "\n",
        "        # crop some of the images by 0-10% of their height/width\n",
        "        # sometimes(iaa.Crop(percent=(0, 0.1))),\n",
        "\n",
        "        # Apply affine transformations to some of the images\n",
        "        # - scale to 80-120% of image height/width (each axis independently)\n",
        "        # - translate by -20 to +20 relative to height/width (per axis)\n",
        "        # - rotate by -45 to +45 degrees\n",
        "        # - shear by -16 to +16 degrees\n",
        "        # - order: use nearest neighbour or bilinear interpolation (fast)\n",
        "        # - mode: use any available mode to fill newly created pixels\n",
        "        #         see API or scikit-image for which modes are available\n",
        "        # - cval: if the mode is constant, then use a random brightness\n",
        "        #         for the newly created pixels (e.g. sometimes black,\n",
        "        #         sometimes white)\n",
        "        sometimes(iaa.Affine(\n",
        "            scale={\"x\": (0.1, 1.1), \"y\": (0.1, 1.1)},\n",
        "            #translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
        "            rotate=(-10, 10),\n",
        "            shear=(-8, 8),\n",
        "            order=[0, 1],\n",
        "            cval=(0, 255),\n",
        "            mode='constant'\n",
        "        )),\n",
        "\n",
        "        #\n",
        "        # Execute 0 to 5 of the following (less important) augmenters per\n",
        "        # image. Don't execute all of them, as that would often be way too\n",
        "        # strong.\n",
        "        #\n",
        "        iaa.SomeOf((0, 3),\n",
        "            [\n",
        "                # Convert some images into their superpixel representation,\n",
        "                # sample between 20 and 200 superpixels per image, but do\n",
        "                # not replace all superpixels with their average, only\n",
        "                # some of them (p_replace).\n",
        "                # sometimes(\n",
        "                #     iaa.Superpixels(\n",
        "                #         p_replace=(0, 1.0),\n",
        "                #         n_segments=(20, 200)\n",
        "                #     )\n",
        "                # ),\n",
        "\n",
        "                # Blur each image with varying strength using\n",
        "                # gaussian blur (sigma between 0 and 3.0),\n",
        "                # average/uniform blur (kernel size between 2x2 and 7x7)\n",
        "                # median blur (kernel size between 3x3 and 11x11).\n",
        "                iaa.OneOf([\n",
        "                    iaa.GaussianBlur((0, 1.0)),\n",
        "                    iaa.AverageBlur(k=(1, 3)),\n",
        "                    iaa.MedianBlur(k=(1, 3)),\n",
        "                ]),\n",
        "\n",
        "                # Sharpen each image, overlay the result with the original\n",
        "                # image using an alpha between 0 (no sharpening) and 1\n",
        "                # (full sharpening effect).\n",
        "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
        "\n",
        "                # Same as sharpen, but for an embossing effect.\n",
        "                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "\n",
        "                # Search in some images either for all edges or for\n",
        "                # directed edges. These edges are then marked in a black\n",
        "                # and white image and overlayed with the original image\n",
        "                # using an alpha of 0 to 0.7.\n",
        "                sometimes(iaa.OneOf([\n",
        "                    iaa.EdgeDetect(alpha=(0, 0.25)),\n",
        "                    iaa.DirectedEdgeDetect(\n",
        "                        alpha=(0, 0.5), direction=(0.0, 0.2)\n",
        "                    ),\n",
        "                ])),\n",
        "\n",
        "                # Add gaussian noise to some images.\n",
        "                # In 50% of these cases, the noise is randomly sampled per\n",
        "                # channel and pixel.\n",
        "                # In the other 50% of all cases it is sampled once per\n",
        "                # pixel (i.e. brightness change).\n",
        "                iaa.AdditiveGaussianNoise(\n",
        "                    loc=0, scale=(0.0, 0.03*255), per_channel=0.5\n",
        "                ),\n",
        "\n",
        "                # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
        "                # them to black) or drop them on an image with 2-5% percent\n",
        "                # of the original size, leading to large dropped\n",
        "                # rectangles.\n",
        "                iaa.OneOf([\n",
        "                    iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                    iaa.CoarseDropout(\n",
        "                        (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                        per_channel=0.2\n",
        "                    ),\n",
        "                ]),\n",
        "\n",
        "                # Invert each image's channel with 5% probability.\n",
        "                # This sets each pixel value v to 255-v.\n",
        "                iaa.Invert(0.05, per_channel=True), # invert color channels\n",
        "\n",
        "                # Add a value of -10 to 10 to each pixel.\n",
        "                iaa.Add((-10, 10), per_channel=0.5),\n",
        "\n",
        "                # Change brightness of images (50-150% of original value).\n",
        "                iaa.Multiply((0.7, 1.2), per_channel=0.5),\n",
        "\n",
        "                # Improve or worsen the contrast of images.\n",
        "                iaa.LinearContrast((0.5, 2.0), per_channel=0.5),\n",
        "\n",
        "                # Convert each image to grayscale and then overlay the\n",
        "                # result with the original with random alpha. I.e. remove\n",
        "                # colors with varying strengths.\n",
        "                iaa.Grayscale(alpha=(0.0, 1.0)),\n",
        "\n",
        "                # In some images move pixels locally around (with random\n",
        "                # strengths).\n",
        "                sometimes(\n",
        "                    iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)\n",
        "                ),\n",
        "\n",
        "                # In some images distort local areas with varying strength.\n",
        "                # sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "\n",
        "                sometimes(iaa.JpegCompression(compression=(70, 99)))\n",
        "            ],\n",
        "            # do all of the above augmentations in random order\n",
        "            random_order=True\n",
        "        )\n",
        "    ],\n",
        "    # do all of the above augmentations in random order\n",
        "    random_order=True\n",
        ")"
      ],
      "metadata": {
        "id": "qttqz1xZkJhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMz6FAkjr13"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_labels, validation_labels = sklearn.model_selection.train_test_split(train_labels, test_size=0.2, random_state=42)\n",
        "(training_image_gen, training_steps), (validation_image_gen, validation_steps) = [\n",
        "    (\n",
        "        keras_ocr.datasets.get_recognizer_image_generator(\n",
        "            labels=labels,\n",
        "            height=recognizer.model.input_shape[1],\n",
        "            width=recognizer.model.input_shape[2],\n",
        "            alphabet=recognizer.alphabet,\n",
        "            augmenter=augmenter\n",
        "        ),\n",
        "        len(labels) // batch_size\n",
        "    ) for labels, augmenter in [(train_labels, augmenter), (validation_labels, None)]\n",
        "]\n",
        "training_gen, validation_gen = [\n",
        "    recognizer.get_batch_generator(\n",
        "        image_generator=image_generator,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    for image_generator in [training_image_gen, validation_image_gen]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKG7BB_4_2tI"
      },
      "source": [
        "As a sanity check, we show one of the samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2eHTeZW-lFt"
      },
      "source": [
        "image, text = next(training_image_gen)\n",
        "print('text:', text)\n",
        "_ = plt.imshow(image)\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_xEBogs_4cK"
      },
      "source": [
        "\n",
        "Now we can run training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwfxrcKFjtxi"
      },
      "source": [
        "callbacks = [\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, restore_best_weights=False),\n",
        "    tf.keras.callbacks.ModelCheckpoint('meeter_rec.h5', monitor='loss', save_best_only=True),\n",
        "    tf.keras.callbacks.CSVLogger('meeter_rec.csv')\n",
        "]\n",
        "\n",
        "history = recognizer.training_model.fit(\n",
        "    x=training_gen,\n",
        "    steps_per_epoch=training_steps,\n",
        "    validation_steps=validation_steps,\n",
        "    validation_data=validation_gen,\n",
        "    callbacks=callbacks,\n",
        "    epochs=250,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPGLJuLg_6iF"
      },
      "source": [
        "Finally, run inference on a test sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNPIzN7kdvY"
      },
      "source": [
        "import random as rnd\n",
        "image_filepath, _, actual = validation_labels[rnd.randint(0, len(validation_labels))]\n",
        "predicted = recognizer.recognize(image_filepath)\n",
        "print(f'Predicted: {predicted}, Actual: {actual}')\n",
        "_ = plt.imshow(keras_ocr.tools.read(image_filepath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "nansjFbp28YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16, 12), dpi=120)\n",
        "results, labels, predictions = [], [], []\n",
        "i = 0\n",
        "\n",
        "for image_filepath, _, actual in validation_labels:\n",
        "  actual = actual\n",
        "  predicted = recognizer.recognize(image_filepath)\n",
        "  results.append(predicted == actual)\n",
        "\n",
        "  if len(actual) == len(predicted):\n",
        "    labels.extend(list(actual))\n",
        "    predictions.extend(list(predicted))\n",
        "\n",
        "  if predicted != actual:\n",
        "    img = Image.open(image_filepath)\n",
        "    plt.subplot(8,4, i+1)\n",
        "    plt.title((predicted,actual))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    i+=1\n",
        "\n",
        "print(f'Validaton Accuracy: {sum(results)/len(results)}')"
      ],
      "metadata": {
        "id": "iKNeMfOm3AMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(12, 8), dpi=80)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = 100*cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.0f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SUlg3WqG2SZG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "target_labels=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.']\n",
        "cf_matrix = confusion_matrix(labels, predictions, labels=target_labels)\n",
        "\n",
        "plot_confusion_matrix(cm=cf_matrix,\n",
        "                      target_names=target_labels,\n",
        "                      normalize=True)"
      ],
      "metadata": {
        "id": "wioZFw_9qn2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TFLite convertion"
      ],
      "metadata": {
        "id": "TyeIxcjGgmRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tflite(quantization, prediction_model):\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "  ]\n",
        "  if quantization == 'float16':\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "  # elif quantization == 'int8' or quantization == 'full_int8':\n",
        "  #   converter.representative_dataset = representative_data_gen\n",
        "  if quantization == 'full_int8':\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "  tf_lite_model = converter.convert()\n",
        "  open(f'meeter_rec_{quantization}.tflite', 'wb').write(tf_lite_model)"
      ],
      "metadata": {
        "id": "h850o4gSgkgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for quantization in ['float16']:\n",
        "  convert_tflite(quantization, recognizer.prediction_model)"
      ],
      "metadata": {
        "id": "0iEfOO2Hhh_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}